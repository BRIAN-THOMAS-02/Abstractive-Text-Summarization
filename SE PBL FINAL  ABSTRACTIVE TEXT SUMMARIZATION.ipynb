{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2562396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5add3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3310bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [\"hello world\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0371fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILEDATA :  ['Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.']\n",
      "\n",
      "\n",
      "ARTICLE :  ['Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming', 'More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed', 'Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g', 'containing words or structures that have not been seen before) and to erroneous input (e.g', 'with misspelled words or words accidentally omitted)', 'Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming', 'Systems based on automatically learning the rules can be made more accurate simply by supplying more input data', 'However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task', 'In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable', 'However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process', 'Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.']\n",
      "\n",
      "\n",
      "\n",
      "DataType of sentences :  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "file = open('testing.txt',\"r\")\n",
    "\n",
    "filedata = file.readlines() \n",
    "print(\"FILEDATA : \", filedata)\n",
    "\n",
    "article = filedata[-1].split(\". \")\n",
    "print(\"\\n\\nARTICLE : \", article)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "print(\"\\n\\n\\nDataType of sentences : \",type(sentences))\n",
    "\n",
    "for sentence in article:\n",
    "    #print(\"\\n\",sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\",\" \").split(\" \"))\n",
    "\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d441be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n",
      "\n",
      "\n",
      "\n",
      "['Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.']\n"
     ]
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in range(len(filedata)):\n",
    "    a = filedata[i].replace(\"\\n\",\" \")\n",
    "    z.append(a)\n",
    "    #a = filedata[i].split(\". \")\n",
    "    #z.append(a)\n",
    "    \n",
    "    print(a)\n",
    "    print('\\n\\n')\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3b58885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n"
     ]
    }
   ],
   "source": [
    "print(len(z))\n",
    "\n",
    "for i in z:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "94e5ce92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='testing.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('testing.txt')\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa53637",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2d2b8",
   "metadata": {},
   "source": [
    "here the <u>article = filedata[0] .split(\".\")</u>\n",
    "\n",
    "the '.' is acting as a delimiter i.e., the words before '.' is one sentence and after '.' is another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5351f23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_article(file_name):\n",
    "    file = open(file_name,\"r\")\n",
    "    filedata = file.readlines() \n",
    "    \n",
    "    z = []\n",
    "    print(len(filedata))\n",
    "\n",
    "    for i in range(len(filedata)):\n",
    "        a = filedata[i].replace(\"\\n\",\" \")\n",
    "        z.append(a)\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in z:\n",
    "        #print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\",\" \"))#.split(\" \"))\n",
    "\n",
    "    #sentences.pop()\n",
    "    print(*sentences)\n",
    "    return sentences\n",
    "\n",
    "file_name = \"testing.txt\"\n",
    "read_article(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8a5bad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower()for w in sent1]\n",
    "    sent2 = [w.lower()for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1+sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [1] * len(all_words)\n",
    "\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "    return 1-cosine_distance(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57480e7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2415f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Methods:', 'Rules,', 'statistics,', 'neural', 'networks', 'In', 'the', 'early', 'days,', 'many', 'language-processing', 'systems', 'were', 'designed', 'by', 'symbolic', 'methods,', 'i.e.,', 'the', 'hand-coding', 'of', 'a', 'set', 'of', 'rules,', 'coupled', 'with', 'a', 'dictionary', 'lookup:[13][14]', 'such', 'as', 'by', 'writing', 'grammars', 'or', 'devising', 'heuristic', 'rules', 'for', 'stemming'], ['More', 'recent', 'systems', 'based', 'on', 'machine-learning', 'algorithms', 'have', 'many', 'advantages', 'over', 'hand-produced', 'rules:', 'The', 'learning', 'procedures', 'used', 'during', 'machine', 'learning', 'automatically', 'focus', 'on', 'the', 'most', 'common', 'cases,', 'whereas', 'when', 'writing', 'rules', 'by', 'hand', 'it', 'is', 'often', 'not', 'at', 'all', 'obvious', 'where', 'the', 'effort', 'should', 'be', 'directed'], ['Automatic', 'learning', 'procedures', 'can', 'make', 'use', 'of', 'statistical', 'inference', 'algorithms', 'to', 'produce', 'models', 'that', 'are', 'robust', 'to', 'unfamiliar', 'input', '(e.g'], ['containing', 'words', 'or', 'structures', 'that', 'have', 'not', 'been', 'seen', 'before)', 'and', 'to', 'erroneous', 'input', '(e.g'], ['with', 'misspelled', 'words', 'or', 'words', 'accidentally', 'omitted)'], ['Generally,', 'handling', 'such', 'input', 'gracefully', 'with', 'handwritten', 'rules,', 'or,', 'more', 'generally,', 'creating', 'systems', 'of', 'handwritten', 'rules', 'that', 'make', 'soft', 'decisions,', 'is', 'extremely', 'difficult,', 'error-prone', 'and', 'time-consuming'], ['Systems', 'based', 'on', 'automatically', 'learning', 'the', 'rules', 'can', 'be', 'made', 'more', 'accurate', 'simply', 'by', 'supplying', 'more', 'input', 'data'], ['However,', 'systems', 'based', 'on', 'handwritten', 'rules', 'can', 'only', 'be', 'made', 'more', 'accurate', 'by', 'increasing', 'the', 'complexity', 'of', 'the', 'rules,', 'which', 'is', 'a', 'much', 'more', 'difficult', 'task'], ['In', 'particular,', 'there', 'is', 'a', 'limit', 'to', 'the', 'complexity', 'of', 'systems', 'based', 'on', 'handwritten', 'rules,', 'beyond', 'which', 'the', 'systems', 'become', 'more', 'and', 'more', 'unmanageable'], ['However,', 'creating', 'more', 'data', 'to', 'input', 'to', 'machine-learning', 'systems', 'simply', 'requires', 'a', 'corresponding', 'increase', 'in', 'the', 'number', 'of', 'man-hours', 'worked,', 'generally', 'without', 'significant', 'increases', 'in', 'the', 'complexity', 'of', 'the', 'annotation', 'process'], ['Despite', 'the', 'popularity', 'of', 'machine', 'learning', 'in', 'NLP', 'research,', 'symbolic', 'methods', 'are', 'still', '(2020)', 'commonly', 'used:', 'when', 'the', 'amount', 'of', 'training', 'data', 'is', 'insufficient', 'to', 'successfully', 'apply', 'machine', 'learning', 'methods,', 'e.g.,', 'for', 'the', 'machine', 'translation', 'of', 'low-resource', 'languages', 'such', 'as', 'provided', 'by', 'the', 'Apertium', 'system,', 'for', 'preprocessing', 'in', 'NLP', 'pipelines,', 'e.g.,', 'tokenization,', 'or', 'for', 'postprocessing', 'and', 'transforming', 'the', 'output', 'of', 'NLP', 'pipelines,', 'e.g.,', 'for', 'knowledge', 'extraction', 'from', 'syntactic', 'parses.']] HELLOOOOOOOOOOOOOOOOOOOOOOOOOO ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "\n",
      "ALL WORDS :  ['4', 'f', 'x', 'c', 'z', 'i', '3', 'k', 'n', '\"', ']', ':', \"'\", 'o', 'e', '[', 'h', 'g', 'p', 'v', '(', ')', 'l', 'b', 'j', 'q', 'a', ',', '.', 'r', 'm', '1', '-', '2', 'd', 's', '0', 'u', ' ', 'y', 't', 'w']\n",
      "Vector 1 :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Vector 2 :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9790558344834829"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    print(sent1, \"HELLOOOOOOOOOOOOOOOOOOOOOOOOOO\", sent2)\n",
    "    \n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "        \n",
    "    sent1 = [w.lower() for w in str(sent1)]\n",
    "    sent2 = [w.lower() for w in str(sent2)]\n",
    "    \n",
    "    #print(\"W : \", w)\n",
    "\n",
    "    all_words = list(set(sent1+sent2))\n",
    "    print(\"\\n\\nALL WORDS : \", all_words)\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    print(\"Vector 1 : \", vector1)\n",
    "    \n",
    "    vector2 = [0] * len(all_words)\n",
    "    print(\"Vector 2 : \", vector2)\n",
    "\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "        \n",
    "    return 1-cosine_distance(vector1,vector2)\n",
    "\n",
    "\n",
    "sentence_similarity(sentences, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbae9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf3ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ebc7963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text : \n",
      "Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n",
      "\n",
      "\n",
      "SUMMARY \n",
      " However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed\n"
     ]
    }
   ],
   "source": [
    "def read_article(file_name):\n",
    "    \n",
    "    file = open(file_name,\"r\")\n",
    "    filedata = file.readlines() \n",
    "    print(\"Raw Text : \")\n",
    "    print(*filedata)\n",
    "    \n",
    "    \"\"\"z = []\n",
    "    print(len(filedata))\n",
    "\n",
    "    for i in range(len(filedata)):\n",
    "        a = filedata[i].replace(\"\\n\",\" \")\n",
    "        z.append(a)\"\"\"\n",
    "\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(\"\\nArticle - Sentence : \\n\", sentence)\n",
    "        sentences.append(sentence.replace(\"^[a-zA-Z]\",\" \").split(\" \"))\n",
    "\n",
    "    sentences.pop()    #Pop() will remove the last element that is space and make every word individual instead of sentence\n",
    "    \n",
    "    #print(\"\\n\\n\")\n",
    "    #print(*sentences)\n",
    "    #print(\"\\n\\n\")\n",
    "    return sentences\n",
    "\n",
    "file_name = \"testing.txt\"\n",
    "#read_article(file_name)\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower()for w in sent1]\n",
    "    sent2 = [w.lower()for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1+sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [1] * len(all_words)\n",
    "\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "    return 1-cosine_distance(vector1,vector2)\n",
    "\n",
    "\n",
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "  \n",
    "    return similarity_matrix  \n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):  #top_n=5 means five lines \n",
    "    stop_words = stopwords.words('English')\n",
    "    summarize_text=[]\n",
    "    sentences = read_article(file_name)\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentence = sorted(((scores[i],s)for i,s in enumerate(sentences)),reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        \n",
    "    print(\"\\n\\nSUMMARY \\n\", \". \".join(summarize_text))\n",
    "    \n",
    "generate_summary(\"testing.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494da68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f276085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_article(file_name):\n",
    "    \n",
    "    file = open(file_name,\"r\")\n",
    "    filedata = file.readlines() \n",
    "    print(\"Raw Text : \")\n",
    "    print(*filedata)\n",
    "    \n",
    "    \"\"\"z = []\n",
    "    print(len(filedata))\n",
    "\n",
    "    for i in range(len(filedata)):\n",
    "        a = filedata[i].replace(\"\\n\",\" \")\n",
    "        z.append(a)\"\"\"\n",
    "\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(\"\\nArticle - Sentence : \\n\", sentence)\n",
    "        sentences.append(sentence.replace(\"^[a-zA-Z]\",\" \").split(\" \"))\n",
    "\n",
    "    sentences.pop()    #Pop() will remove the last element that is space and make every word individual instead of sentence\n",
    "    \n",
    "    #print(\"\\n\\n\")\n",
    "    #print(*sentences)\n",
    "    #print(\"\\n\\n\")\n",
    "    return sentences\n",
    "\n",
    "file_name = \"testing.txt\"\n",
    "#read_article(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cbe79dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower()for w in sent1]\n",
    "    sent2 = [w.lower()for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1+sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [1] * len(all_words)\n",
    "\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "    return 1-cosine_distance(vector1,vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5aa32814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "  \n",
    "    return similarity_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "55ac1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text : \n",
      "Methods: Rules, statistics, neural networks In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming. Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\n",
      "\n",
      "\n",
      "SUMMARY \n",
      " However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(file_name, top_n=5):  #top_n=5 means five lines \n",
    "    stop_words = stopwords.words('English')\n",
    "    summarize_text=[]\n",
    "    sentences = read_article(file_name)\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentence = sorted(((scores[i],s)for i,s in enumerate(sentences)),reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        \n",
    "    print(\"\\n\\nSUMMARY \\n\", \". \".join(summarize_text))\n",
    "    \n",
    "generate_summary(\"testing.txt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad515d9",
   "metadata": {},
   "source": [
    "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup. More recent systems based on machine-learning algorithms have many advantages over hand-produced rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b17a1635",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f6a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a56b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "91d1f35a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '10' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\iSkysoft\\CreatorTemp/ipykernel_1200/1969316783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SUMMARY \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\". \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mgenerate_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"testing.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\iSkysoft\\CreatorTemp/ipykernel_1200/1969316783.py\u001b[0m in \u001b[0;36mgenerate_summary\u001b[1;34m(file_name, top_n)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0msentence_similarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_sim_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0msentence_similarity_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\iSkysoft\\CreatorTemp/ipykernel_1200/1969316783.py\u001b[0m in \u001b[0;36mgen_sim_matrix\u001b[1;34m(sentences, stop_words)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_sim_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0msimilarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '10' as a data type"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def read_article(file_name):\n",
    "    file = open(file_name,\"r\")\n",
    "    filedata = file.readlines() \n",
    "    \n",
    "    article = filedata[0].split(\". \")\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\",\" \").split(\" \"))\n",
    "\n",
    "    sentences.pop()\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower()for w in sent1]\n",
    "    sent2 = [w.lower()for w in sent2]\n",
    "\n",
    "    all_words = list(set(sent1+sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [1] * len(all_words)\n",
    "\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] +=1\n",
    "\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] +=1\n",
    "    return 1-cosine_distance(vector1,vector2)\n",
    "\n",
    "\n",
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros(len(sentences), len(sentences))\n",
    "\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "  \n",
    "    return similarity_matrix  \n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('English')\n",
    "    summarize_text=[]\n",
    "    \n",
    "    sentences = read_article(file_name)\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentence = sorted(((scores[i],s)for i,s in enumerate(sentences)),reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
    "        \n",
    "    print(\"SUMMARY \\n\", \". \".join(summarize_text))\n",
    "    \n",
    "generate_summary(\"testing.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daa8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
